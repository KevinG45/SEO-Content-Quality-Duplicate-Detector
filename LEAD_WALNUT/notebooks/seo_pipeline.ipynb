{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa18dfc9",
   "metadata": {},
   "source": [
    "# SEO Content Quality & Duplicate Detector\n",
    "## Complete ML Pipeline for Web Content Analysis\n",
    "\n",
    "**Author:** LEAD_WALNUT Project  \n",
    "**Date:** November 3, 2025  \n",
    "**Objective:** Build a machine learning pipeline that analyzes web content for SEO quality assessment and duplicate detection.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements:\n",
    "1. ✅ **HTML Parsing** - Extract clean text from HTML content\n",
    "2. ✅ **Feature Engineering** - Calculate readability, keywords, embeddings\n",
    "3. ✅ **Duplicate Detection** - Identify similar content using cosine similarity\n",
    "4. ✅ **Quality Classification** - Train ML model to score content quality\n",
    "5. ✅ **Real-Time Analysis** - Function to analyze any URL on-demand\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37b16c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n",
      "Working directory: c:\\Users\\kmgs4\\Documents\\Christ Uni\\trimester-5\\LEAD_WALNUT\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# HTML Parsing\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "# NLP & Text Processing\n",
    "import textstat\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"All libraries imported successfully.\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ab3f1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    print(\"NLTK data downloaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: NLTK download issue - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563fdec4",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Load the CSV file containing URLs and HTML content (65 rows sampled from the original dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "101b031b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Shape: (65, 2)\n",
      "Columns: ['url', 'html_content']\n",
      "\n",
      "Sample URLs:\n",
      "  1. https://comfax.com/reviews/free-fax/...\n",
      "  2. https://www.cm-alliance.com/cybersecurity-blog...\n",
      "  3. https://en.wikipedia.org/wiki/Remote_desktop_software...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/data.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully.\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nSample URLs:\")\n",
    "for i, url in enumerate(df['url'].head(3)):\n",
    "    print(f\"  {i+1}. {url[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "132d843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_html() function defined.\n"
     ]
    }
   ],
   "source": [
    "def parse_html(html_content):\n",
    "    \"\"\"\n",
    "    Parse HTML content and extract clean text information.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing title, body_text, and word_count\n",
    "              Returns None if parsing fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        \n",
    "        # Extract page title\n",
    "        title = soup.find('title')\n",
    "        title_text = title.get_text().strip() if title else \"No title\"\n",
    "        \n",
    "        # Remove non-content elements\n",
    "        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'form']):\n",
    "            tag.decompose()\n",
    "        \n",
    "        # Extract body text\n",
    "        body = soup.find('body')\n",
    "        if body:\n",
    "            text = body.get_text(separator=' ', strip=True)\n",
    "        else:\n",
    "            text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Clean text by removing extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Calculate word count\n",
    "        word_count = len(text.split())\n",
    "        \n",
    "        return {\n",
    "            'title': title_text,\n",
    "            'body_text': text,\n",
    "            'word_count': word_count\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Parsing error - {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"parse_html() function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abace289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HTML PARSING\n",
      "============================================================\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "Warning: Parsing error - Incoming markup is of an invalid type: nan. Markup must be a string, a bytestring, or an open filehandle.\n",
      "\n",
      "Parsing Results:\n",
      "  Successfully parsed: 57 pages\n",
      "  Failed to parse: 8 pages\n",
      "  Saved to: data/extracted_content.csv\n",
      "\n",
      "Sample extracted content:\n",
      "                                                 url  \\\n",
      "0               https://comfax.com/reviews/free-fax/   \n",
      "1     https://www.cm-alliance.com/cybersecurity-blog   \n",
      "2  https://en.wikipedia.org/wiki/Remote_desktop_s...   \n",
      "3  https://nytlicensing.com/latest/trends/content...   \n",
      "4      https://sign.dropbox.com/products/dropbox-fax   \n",
      "\n",
      "                                               title  word_count  \n",
      "0  Free Fax Review - Top 7 Choices Reviewed and T...        5094  \n",
      "1                                Cyber Security Blog        2017  \n",
      "2                Remote desktop software - Wikipedia        2165  \n",
      "3  16 Best Practices for Content Marketing in 202...        3284  \n",
      "4  Send and Receive Fax Online by Phone and Compu...         490  \n",
      "\n",
      "Parsing Results:\n",
      "  Successfully parsed: 57 pages\n",
      "  Failed to parse: 8 pages\n",
      "  Saved to: data/extracted_content.csv\n",
      "\n",
      "Sample extracted content:\n",
      "                                                 url  \\\n",
      "0               https://comfax.com/reviews/free-fax/   \n",
      "1     https://www.cm-alliance.com/cybersecurity-blog   \n",
      "2  https://en.wikipedia.org/wiki/Remote_desktop_s...   \n",
      "3  https://nytlicensing.com/latest/trends/content...   \n",
      "4      https://sign.dropbox.com/products/dropbox-fax   \n",
      "\n",
      "                                               title  word_count  \n",
      "0  Free Fax Review - Top 7 Choices Reviewed and T...        5094  \n",
      "1                                Cyber Security Blog        2017  \n",
      "2                Remote desktop software - Wikipedia        2165  \n",
      "3  16 Best Practices for Content Marketing in 202...        3284  \n",
      "4  Send and Receive Fax Online by Phone and Compu...         490  \n"
     ]
    }
   ],
   "source": [
    "# Parse HTML content for all pages\n",
    "print(\"=\" * 60)\n",
    "print(\"HTML PARSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "extracted_data = []\n",
    "failed_count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    parsed = parse_html(row['html_content'])\n",
    "    if parsed:\n",
    "        extracted_data.append({\n",
    "            'url': row['url'],\n",
    "            'title': parsed['title'],\n",
    "            'body_text': parsed['body_text'],\n",
    "            'word_count': parsed['word_count']\n",
    "        })\n",
    "    else:\n",
    "        failed_count += 1\n",
    "\n",
    "# Create DataFrame from extracted data\n",
    "extracted_df = pd.DataFrame(extracted_data)\n",
    "\n",
    "# Save extracted content (without HTML for smaller file size)\n",
    "extracted_df.to_csv('../data/extracted_content.csv', index=False)\n",
    "\n",
    "print(f\"\\nParsing Results:\")\n",
    "print(f\"  Successfully parsed: {len(extracted_data)} pages\")\n",
    "print(f\"  Failed to parse: {failed_count} pages\")\n",
    "print(f\"  Saved to: data/extracted_content.csv\")\n",
    "\n",
    "# Display sample of extracted content\n",
    "print(f\"\\nSample extracted content:\")\n",
    "print(extracted_df[['url', 'title', 'word_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "450bd6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n",
      "Generating embeddings (this may take a moment)...\n",
      "Generating embeddings (this may take a moment)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b614c62acda48868fe3bf540b017b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete. Shape: (57, 384)\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction functions\n",
    "def extract_sentence_count(text):\n",
    "    \"\"\"Extract sentence count from text using NLTK tokenizer.\"\"\"\n",
    "    try:\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        return len(sentences)\n",
    "    except:\n",
    "        return len([s for s in text.split('.') if s.strip()])\n",
    "\n",
    "def extract_readability(text):\n",
    "    \"\"\"Calculate Flesch Reading Ease score for text.\"\"\"\n",
    "    try:\n",
    "        return textstat.flesch_reading_ease(text)\n",
    "    except:\n",
    "        return 50.0\n",
    "\n",
    "def extract_top_keywords(texts, n_keywords=5):\n",
    "    \"\"\"Extract top N keywords using TF-IDF vectorization.\"\"\"\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=100, \n",
    "            stop_words='english', \n",
    "            max_df=0.85, \n",
    "            min_df=2\n",
    "        )\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        keywords_list = []\n",
    "        for doc_idx in range(tfidf_matrix.shape[0]):\n",
    "            tfidf_scores = tfidf_matrix[doc_idx].toarray()[0]\n",
    "            top_indices = tfidf_scores.argsort()[-n_keywords:][::-1]\n",
    "            top_keywords = [feature_names[i] for i in top_indices if tfidf_scores[i] > 0]\n",
    "            keywords_list.append('|'.join(top_keywords) if top_keywords else 'no_keywords')\n",
    "        return keywords_list\n",
    "    except:\n",
    "        return ['no_keywords'] * len(texts)\n",
    "\n",
    "print(\"Extracting features...\")\n",
    "\n",
    "# Extract basic features\n",
    "extracted_df['sentence_count'] = extracted_df['body_text'].apply(extract_sentence_count)\n",
    "extracted_df['flesch_reading_ease'] = extracted_df['body_text'].apply(extract_readability)\n",
    "extracted_df['top_keywords'] = extract_top_keywords(extracted_df['body_text'].tolist())\n",
    "\n",
    "# Generate semantic embeddings\n",
    "print(\"Generating embeddings (this may take a moment)...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(extracted_df['body_text'].tolist(), show_progress_bar=True, batch_size=16)\n",
    "extracted_df['embedding'] = list(embeddings)\n",
    "\n",
    "print(f\"Feature extraction complete. Shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20370f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to: data/features.csv\n",
      "\n",
      "Feature statistics:\n",
      "         word_count  sentence_count  flesch_reading_ease\n",
      "count     57.000000       57.000000            57.000000\n",
      "mean    3084.859649      156.403509            38.769881\n",
      "std     3611.009096      192.121600            21.489318\n",
      "min       58.000000        5.000000            -3.257091\n",
      "25%      863.000000       31.000000            24.939930\n",
      "50%     2165.000000       90.000000            37.880668\n",
      "75%     4091.000000      181.000000            52.003378\n",
      "max    22520.000000      970.000000           102.680379\n"
     ]
    }
   ],
   "source": [
    "# Create features dataframe and save to CSV\n",
    "features_df = extracted_df[['url', 'word_count', 'sentence_count', 'flesch_reading_ease', 'top_keywords', 'embedding']].copy()\n",
    "\n",
    "# Convert embeddings to string for CSV storage\n",
    "features_df['embedding_vector'] = features_df['embedding'].apply(lambda x: str(x.tolist()))\n",
    "features_to_save = features_df.drop('embedding', axis=1)\n",
    "\n",
    "# Save features (without embedding column to reduce file size)\n",
    "features_to_save.to_csv('../data/features.csv', index=False)\n",
    "\n",
    "print(f\"Features saved to: data/features.csv\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(features_df[['word_count', 'sentence_count', 'flesch_reading_ease']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7aba8eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarity matrix...\n",
      "\n",
      "Duplicate Detection Summary:\n",
      "  Duplicate pairs found: 2\n",
      "  Thin content pages: 10 (17.5%)\n",
      "  Results saved to: data/duplicates.csv\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity matrix\n",
    "print(\"Computing similarity matrix...\")\n",
    "similarity_matrix = cosine_similarity(np.array(features_df['embedding'].tolist()))\n",
    "\n",
    "# Find duplicate pairs above threshold\n",
    "SIMILARITY_THRESHOLD = 0.80\n",
    "duplicates = []\n",
    "urls = features_df['url'].tolist()\n",
    "\n",
    "for i in range(len(urls)):\n",
    "    for j in range(i+1, len(urls)):\n",
    "        if similarity_matrix[i][j] > SIMILARITY_THRESHOLD:\n",
    "            duplicates.append({\n",
    "                'url1': urls[i],\n",
    "                'url2': urls[j],\n",
    "                'similarity': round(similarity_matrix[i][j], 3)\n",
    "            })\n",
    "\n",
    "# Detect thin content (pages with less than 500 words)\n",
    "features_df['is_thin'] = features_df['word_count'] < 500\n",
    "thin_count = features_df['is_thin'].sum()\n",
    "\n",
    "# Save duplicate pairs to CSV\n",
    "if duplicates:\n",
    "    pd.DataFrame(duplicates).to_csv('../data/duplicates.csv', index=False)\n",
    "else:\n",
    "    pd.DataFrame(columns=['url1', 'url2', 'similarity']).to_csv('../data/duplicates.csv', index=False)\n",
    "\n",
    "print(f\"\\nDuplicate Detection Summary:\")\n",
    "print(f\"  Duplicate pairs found: {len(duplicates)}\")\n",
    "print(f\"  Thin content pages: {thin_count} ({thin_count/len(features_df)*100:.1f}%)\")\n",
    "print(f\"  Results saved to: data/duplicates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fdc85",
   "metadata": {},
   "source": [
    "## 13. Save Trained Model\n",
    "\n",
    "Persist the trained model to disk for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d129ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n",
      "  Baseline Accuracy: 0.444\n",
      "  Random Forest Accuracy: 0.833\n",
      "  Improvement: 87.5%\n"
     ]
    }
   ],
   "source": [
    "def baseline_predict(word_count):\n",
    "    \"\"\"\n",
    "    Simple rule-based baseline classifier using only word count.\n",
    "    \n",
    "    Args:\n",
    "        word_count (int): Number of words in content\n",
    "        \n",
    "    Returns:\n",
    "        str: Predicted quality label\n",
    "    \"\"\"\n",
    "    if word_count > 1000:\n",
    "        return 'High'\n",
    "    elif word_count < 500:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Medium'\n",
    "\n",
    "# Evaluate baseline model\n",
    "baseline_preds = X_test['word_count'].apply(baseline_predict)\n",
    "baseline_accuracy = accuracy_score(y_test, baseline_preds)\n",
    "\n",
    "# Compare baseline vs Random Forest\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"  Baseline Accuracy: {baseline_accuracy:.3f}\")\n",
    "print(f\"  Random Forest Accuracy: {rf_accuracy:.3f}\")\n",
    "print(f\"  Improvement: {((rf_accuracy - baseline_accuracy) / baseline_accuracy * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa588c3",
   "metadata": {},
   "source": [
    "## 12. Baseline Comparison\n",
    "\n",
    "Compare Random Forest performance against simple rule-based baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1f5e5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Performance:\n",
      "Accuracy: 0.833\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       1.00      0.50      0.67         2\n",
      "         Low       0.80      1.00      0.89         8\n",
      "      Medium       0.86      0.75      0.80         8\n",
      "\n",
      "    accuracy                           0.83        18\n",
      "   macro avg       0.89      0.75      0.79        18\n",
      "weighted avg       0.85      0.83      0.82        18\n",
      "\n",
      "\n",
      "Feature Importance:\n",
      "               feature  importance\n",
      "2  flesch_reading_ease    0.497291\n",
      "0           word_count    0.283375\n",
      "1       sentence_count    0.219334\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target for model training\n",
    "X = features_df[['word_count', 'sentence_count', 'flesch_reading_ease']]\n",
    "y = features_df['quality_label']\n",
    "\n",
    "# Split data with stratification to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Train Random Forest classifier\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    max_depth=10\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "y_pred = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Random Forest Model Performance:\")\n",
    "print(f\"Accuracy: {rf_accuracy:.3f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display feature importance\n",
    "feat_imp = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feat_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f768666",
   "metadata": {},
   "source": [
    "## 11. Train Quality Classification Model\n",
    "\n",
    "Train Random Forest classifier to predict content quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e5428fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality labels created.\n",
      "\n",
      "Label distribution:\n",
      "quality_label\n",
      "Medium    25\n",
      "Low       25\n",
      "High       7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_quality_labels(row):\n",
    "    \"\"\"\n",
    "    Create quality labels based on word count and readability metrics.\n",
    "    \n",
    "    Labeling criteria:\n",
    "    - High: word_count > 1500 AND readability between 50-70\n",
    "    - Low: word_count < 500 OR readability < 30\n",
    "    - Medium: all other cases\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with word_count and flesch_reading_ease columns\n",
    "        \n",
    "    Returns:\n",
    "        str: Quality label ('High', 'Medium', or 'Low')\n",
    "    \"\"\"\n",
    "    if row['word_count'] > 1500 and 50 <= row['flesch_reading_ease'] <= 70:\n",
    "        return 'High'\n",
    "    elif row['word_count'] < 500 or row['flesch_reading_ease'] < 30:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Medium'\n",
    "\n",
    "# Apply labeling function to create quality labels\n",
    "features_df['quality_label'] = features_df.apply(create_quality_labels, axis=1)\n",
    "\n",
    "print(\"Quality labels created.\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(features_df['quality_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86cc9c8",
   "metadata": {},
   "source": [
    "## 10. Create Quality Labels\n",
    "\n",
    "Generate quality labels based on content metrics for ML training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ae28da1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: models/quality_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "with open('../models/quality_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "    \n",
    "print(\"Model saved to: models/quality_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa51ae94",
   "metadata": {},
   "source": [
    "## 15. Demo - Test Real-Time Analysis\n",
    "\n",
    "Test the analyze_url() function with a sample URL from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "98ebd39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_url() function defined.\n"
     ]
    }
   ],
   "source": [
    "def analyze_url(url, existing_embeddings=None, existing_urls=None, model=None, similarity_threshold=0.75):\n",
    "    \"\"\"\n",
    "    Analyze a URL for content quality and find similar pages.\n",
    "    \n",
    "    This function scrapes a given URL, extracts features, predicts content quality,\n",
    "    and identifies similar pages based on semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to analyze\n",
    "        existing_embeddings (np.array, optional): Embeddings for similarity comparison\n",
    "        existing_urls (list, optional): List of existing URLs\n",
    "        model (object, optional): Trained classification model\n",
    "        similarity_threshold (float): Threshold for similarity detection (default: 0.75)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Analysis results including quality label, metrics, and similar pages\n",
    "              Returns error dict if analysis fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Scrape URL with appropriate headers\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse HTML content\n",
    "        parsed = parse_html(response.text)\n",
    "        if not parsed:\n",
    "            return {\"error\": \"Failed to parse HTML content\"}\n",
    "        \n",
    "        # Extract features\n",
    "        sentence_count = extract_sentence_count(parsed['body_text'])\n",
    "        readability = extract_readability(parsed['body_text'])\n",
    "        word_count = parsed['word_count']\n",
    "        \n",
    "        # Generate embedding for similarity comparison\n",
    "        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        embedding = embedding_model.encode([parsed['body_text']])[0]\n",
    "        \n",
    "        # Load model if not provided\n",
    "        if model is None:\n",
    "            with open('../models/quality_model.pkl', 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "        \n",
    "        # Predict quality\n",
    "        features = pd.DataFrame({\n",
    "            'word_count': [word_count],\n",
    "            'sentence_count': [sentence_count],\n",
    "            'flesch_reading_ease': [readability]\n",
    "        })\n",
    "        \n",
    "        quality_label = model.predict(features)[0]\n",
    "        quality_proba = model.predict_proba(features)[0]\n",
    "        \n",
    "        # Find similar content if embeddings provided\n",
    "        similar_pages = []\n",
    "        if existing_embeddings is not None and existing_urls is not None:\n",
    "            similarities = cosine_similarity([embedding], existing_embeddings)[0]\n",
    "            for idx, sim in enumerate(similarities):\n",
    "                if sim > similarity_threshold and existing_urls[idx] != url:\n",
    "                    similar_pages.append({\n",
    "                        'url': existing_urls[idx],\n",
    "                        'similarity': round(float(sim), 3)\n",
    "                    })\n",
    "            similar_pages = sorted(similar_pages, key=lambda x: x['similarity'], reverse=True)[:5]\n",
    "        \n",
    "        # Return analysis results\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': parsed['title'],\n",
    "            'word_count': word_count,\n",
    "            'sentence_count': sentence_count,\n",
    "            'readability': round(readability, 2),\n",
    "            'quality_label': quality_label,\n",
    "            'quality_confidence': {\n",
    "                c: round(float(p), 3) \n",
    "                for c, p in zip(model.classes_, quality_proba)\n",
    "            },\n",
    "            'is_thin': word_count < 500,\n",
    "            'similar_to': similar_pages\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Analysis failed: {str(e)}\"}\n",
    "\n",
    "print(\"analyze_url() function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a08b00c",
   "metadata": {},
   "source": [
    "## 14. Real-Time URL Analysis Function\n",
    "\n",
    "Function to analyze any URL for content quality and find similar pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6c472123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE EXECUTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Deliverables created:\n",
      "  1. data/extracted_content.csv - Parsed HTML content\n",
      "  2. data/features.csv - Extracted features\n",
      "  3. data/duplicates.csv - Duplicate pairs\n",
      "  4. models/quality_model.pkl - Trained model\n",
      "  5. analyze_url() function - Real-time analysis\n",
      "\n",
      "Final Statistics:\n",
      "  Pages processed: 57\n",
      "  Duplicate pairs: 2\n",
      "  Thin content: 10 (17.5%)\n",
      "  Model accuracy: 0.833\n",
      "  Baseline accuracy: 0.444\n",
      "  Improvement: 87.5%\n",
      "\n",
      "Next steps:\n",
      "  1. Review generated CSV files in data/ folder\n",
      "  2. Test analyze_url() with different URLs\n",
      "  3. Push to GitHub repository\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE EXECUTION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nDeliverables created:\")\n",
    "print(\"  1. data/extracted_content.csv - Parsed HTML content\")\n",
    "print(\"  2. data/features.csv - Extracted features\")\n",
    "print(\"  3. data/duplicates.csv - Duplicate pairs\")\n",
    "print(\"  4. models/quality_model.pkl - Trained model\")\n",
    "print(\"  5. analyze_url() function - Real-time analysis\")\n",
    "\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"  Pages processed: {len(features_df)}\")\n",
    "print(f\"  Duplicate pairs: {len(duplicates)}\")\n",
    "print(f\"  Thin content: {thin_count} ({thin_count/len(features_df)*100:.1f}%)\")\n",
    "print(f\"  Model accuracy: {rf_accuracy:.3f}\")\n",
    "print(f\"  Baseline accuracy: {baseline_accuracy:.3f}\")\n",
    "print(f\"  Improvement: {((rf_accuracy - baseline_accuracy) / baseline_accuracy * 100):.1f}%\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Review generated CSV files in data/ folder\")\n",
    "print(\"  2. Test analyze_url() with different URLs\")\n",
    "print(\"  3. Push to GitHub repository\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e67a148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sample URL: https://comfax.com/reviews/free-fax/\n",
      "\n",
      "Analysis Results:\n",
      "============================================================\n",
      "{\n",
      "  \"url\": \"https://comfax.com/reviews/free-fax/\",\n",
      "  \"title\": \"Free Fax Review - Top 7 Choices Reviewed and Tested, 2025 Updated\",\n",
      "  \"word_count\": 5094,\n",
      "  \"sentence_count\": 236,\n",
      "  \"readability\": 60.13,\n",
      "  \"quality_label\": \"High\",\n",
      "  \"quality_confidence\": {\n",
      "    \"High\": 0.78,\n",
      "    \"Low\": 0.02,\n",
      "    \"Medium\": 0.2\n",
      "  },\n",
      "  \"is_thin\": false,\n",
      "  \"similar_to\": []\n",
      "}\n",
      "Analysis Results:\n",
      "============================================================\n",
      "{\n",
      "  \"url\": \"https://comfax.com/reviews/free-fax/\",\n",
      "  \"title\": \"Free Fax Review - Top 7 Choices Reviewed and Tested, 2025 Updated\",\n",
      "  \"word_count\": 5094,\n",
      "  \"sentence_count\": 236,\n",
      "  \"readability\": 60.13,\n",
      "  \"quality_label\": \"High\",\n",
      "  \"quality_confidence\": {\n",
      "    \"High\": 0.78,\n",
      "    \"Low\": 0.02,\n",
      "    \"Medium\": 0.2\n",
      "  },\n",
      "  \"is_thin\": false,\n",
      "  \"similar_to\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for real-time analysis demo\n",
    "existing_embeddings = np.array(features_df['embedding'].tolist())\n",
    "existing_urls = features_df['url'].tolist()\n",
    "\n",
    "# Test analyze_url() function with sample URL\n",
    "test_url = features_df['url'].iloc[0]\n",
    "\n",
    "print(f\"Analyzing sample URL: {test_url}\\n\")\n",
    "\n",
    "result = analyze_url(\n",
    "    test_url, \n",
    "    existing_embeddings=existing_embeddings,\n",
    "    existing_urls=existing_urls,\n",
    "    model=rf_model,\n",
    "    similarity_threshold=0.75\n",
    ")\n",
    "\n",
    "print(\"Analysis Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nndl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
